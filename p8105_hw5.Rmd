---
title: "Untitled"
author: "Tai Yue"
date: "2024-11-11"
output:
  pdf_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

problem1
```{r}
has_shared_birthday <- function(n) {
 
  birthdays <- sample(1:365, n, replace = TRUE)
  
  
  return(length(birthdays) != length(unique(birthdays)))
}

```

```{r}
has_shared_birthday <- function(n) {
  birthdays <- sample(1:365, n, replace = TRUE)
  return(length(birthdays) != length(unique(birthdays)))
}


group_sizes <- 2:50
n_simulations <- 10000
probabilities <- numeric(length(group_sizes))


for (i in seq_along(group_sizes)) {
  n <- group_sizes[i]
  duplicates <- sum(replicate(n_simulations, has_shared_birthday(n)))
  probabilities[i] <- duplicates / n_simulations
}


plot(group_sizes, probabilities, type = "o", pch = 19,
     xlab = "Group Size", ylab = "Probability of Shared Birthday",
     main = "Probability of Shared Birthday as a Function of Group Size")
grid()

```

problem2:

```{r}
library(broom)
library(ggplot2)


n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
num_simulations <- 5000


results <- data.frame()

for (mu in mu_values) {
  
  rejections <- 0
  
  for (i in 1:num_simulations) {
   
    sample <- rnorm(n, mean = mu, sd = sigma)
    
    
    t_test_result <- t.test(sample, mu = 0)
    tidy_result <- tidy(t_test_result)
    
    
    if (tidy_result$p.value < alpha) {
      rejections <- rejections + 1
    }
  }
  
  
  power <- rejections / num_simulations
  
  
  results <- rbind(results, data.frame(mu = mu, power = power))
}


ggplot(results, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of μ",
    y = "Power (Proportion of Null Rejections)",
    title = "Power of One-Sample t-Test vs. Effect Size (μ)"
  ) +
  theme_minimal()

```

For small values of μ, the power is low, meaning the test often fails to reject the null hypothesis. As μ increases, the power rises significantly, reaching close to 1 for μ values around 4 and higher. This means that, with larger effect sizes, the test almost always correctly rejects the null hypothesis.The trend suggests a positive relationship between effect size and power: larger effect sizes make it easier to detect an effect, thus increasing the test's power.

```{r}
library(broom)
library(ggplot2)
library(dplyr)

# Set parameters
n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
num_simulations <- 5000

# Initialize results storage
results <- data.frame()

for (mu in mu_values) {
  # Run simulations and collect both estimate and rejection status
  estimates <- numeric(num_simulations)
  rejections <- numeric(num_simulations)
  
  for (i in 1:num_simulations) {
    # Generate a sample
    sample <- rnorm(n, mean = mu, sd = sigma)
    
    # Perform t-test
    t_test_result <- t.test(sample, mu = 0)
    tidy_result <- tidy(t_test_result)
    
    # Store the estimate and rejection status
    estimates[i] <- tidy_result$estimate
    rejections[i] <- ifelse(tidy_result$p.value < alpha, 1, 0)
  }
  
  # Calculate average estimate of mu_hat
  avg_mu_hat <- mean(estimates)
  
  # Calculate average estimate of mu_hat for rejected tests only
  avg_mu_hat_rejected <- mean(estimates[rejections == 1])
  
  # Store results
  results <- rbind(results, data.frame(mu = mu, avg_mu_hat = avg_mu_hat, avg_mu_hat_rejected = avg_mu_hat_rejected))
}

# Plot both averages
ggplot(results, aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat), color = "blue", linetype = "solid") +
  geom_point(aes(y = avg_mu_hat), color = "blue") +
  geom_line(aes(y = avg_mu_hat_rejected), color = "red", linetype = "dashed") +
  geom_point(aes(y = avg_mu_hat_rejected), color = "red") +
  labs(
    x = "True Value of μ",
    y = "Average Estimate of μ̂",
    title = "Average Estimate of μ̂ vs. True Value of μ",
    subtitle = "Blue: Overall Average, Red: Average for Null Rejected"
  ) +
  theme_minimal() +
  scale_y_continuous(sec.axis = dup_axis(name = "Average Estimate of μ̂"))

```

the red dashed line is very close to the true value of μ .The average estimate of μ^ across tests where the null is rejected approximates the true value of μ well for larger effect sizes but overestimates it for smaller values of μ due to selection bias among the rejected samples.

