---
title: "Untitled"
author: "Tai Yue"
date: "2024-11-11"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

problem1
```{r}
has_shared_birthday <- function(n) {
 
  birthdays <- sample(1:365, n, replace = TRUE)
  
  
  return(length(birthdays) != length(unique(birthdays)))
}

```

```{r}
has_shared_birthday <- function(n) {
  birthdays <- sample(1:365, n, replace = TRUE)
  return(length(birthdays) != length(unique(birthdays)))
}


group_sizes <- 2:50
n_simulations <- 10000
probabilities <- numeric(length(group_sizes))


for (i in seq_along(group_sizes)) {
  n <- group_sizes[i]
  duplicates <- sum(replicate(n_simulations, has_shared_birthday(n)))
  probabilities[i] <- duplicates / n_simulations
}


plot(group_sizes, probabilities, type = "o", pch = 19,
     xlab = "Group Size", ylab = "Probability of Shared Birthday",
     main = "Probability of Shared Birthday as a Function of Group Size")
grid()

```

problem2:

```{r}
library(broom)
library(ggplot2)


n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
num_simulations <- 5000


results <- data.frame()

for (mu in mu_values) {
  
  rejections <- 0
  
  for (i in 1:num_simulations) {
   
    sample <- rnorm(n, mean = mu, sd = sigma)
    
    
    t_test_result <- t.test(sample, mu = 0)
    tidy_result <- tidy(t_test_result)
    
    
    if (tidy_result$p.value < alpha) {
      rejections <- rejections + 1
    }
  }
  
  
  power <- rejections / num_simulations
  
  
  results <- rbind(results, data.frame(mu = mu, power = power))
}


ggplot(results, aes(x = mu, y = power)) +
  geom_line() +
  geom_point() +
  labs(
    x = "True Value of μ",
    y = "Power (Proportion of Null Rejections)",
    title = "Power of One-Sample t-Test vs. Effect Size (μ)"
  ) +
  theme_minimal()

```

For small values of μ, the power is low, meaning the test often fails to reject the null hypothesis. As μ increases, the power rises significantly, reaching close to 1 for μ values around 4 and higher. This means that, with larger effect sizes, the test almost always correctly rejects the null hypothesis.The trend suggests a positive relationship between effect size and power: larger effect sizes make it easier to detect an effect, thus increasing the test's power.

```{r}
library(broom)
library(ggplot2)
library(dplyr)


n <- 30
sigma <- 5
alpha <- 0.05
mu_values <- c(0, 1, 2, 3, 4, 5, 6)
num_simulations <- 5000


results <- data.frame()

for (mu in mu_values) {
  
  estimates <- numeric(num_simulations)
  rejections <- numeric(num_simulations)
  
  for (i in 1:num_simulations) {
    
    sample <- rnorm(n, mean = mu, sd = sigma)
    
    
    t_test_result <- t.test(sample, mu = 0)
    tidy_result <- tidy(t_test_result)
    
    
    estimates[i] <- tidy_result$estimate
    rejections[i] <- ifelse(tidy_result$p.value < alpha, 1, 0)
  }
  
  
  avg_mu_hat <- mean(estimates)
  
  
  avg_mu_hat_rejected <- mean(estimates[rejections == 1])
  
  
  results <- rbind(results, data.frame(mu = mu, avg_mu_hat = avg_mu_hat, avg_mu_hat_rejected = avg_mu_hat_rejected))
}


ggplot(results, aes(x = mu)) +
  geom_line(aes(y = avg_mu_hat), color = "blue", linetype = "solid") +
  geom_point(aes(y = avg_mu_hat), color = "blue") +
  geom_line(aes(y = avg_mu_hat_rejected), color = "red", linetype = "dashed") +
  geom_point(aes(y = avg_mu_hat_rejected), color = "red") +
  labs(
    x = "True Value of μ",
    y = "Average Estimate of μ̂",
    title = "Average Estimate of μ̂ vs. True Value of μ",
    subtitle = "Blue: Overall Average, Red: Average for Null Rejected"
  ) +
  theme_minimal() +
  scale_y_continuous(sec.axis = dup_axis(name = "Average Estimate of μ̂"))

```

the red dashed line is very close to the true value of μ .The average estimate of μ^ across tests where the null is rejected approximates the true value of μ well for larger effect sizes but overestimates it for smaller values of μ due to selection bias among the rejected samples.

problem3:

The raw dataset on homicides in large U.S. cities includes the following fields:

uid: A unique identifier for each homicide case.
reported_date: The date the homicide was reported, formatted as an eight-digit integer 
victim_last: The last name of the victim.
victim_first: The first name of the victim.
victim_race: The race of the victim 
victim_age: The age of the victim.
victim_sex: The sex of the victim, usually Male or Female.
city: The city where the homicide occurred.
state: The state where the homicide occurred.
lat: The latitude of the homicide location.
lon: The longitude of the homicide location.
disposition: The outcome or current status of the case.
city_state: A derived field combining the city and state for each record, added during the analysis for grouping purposes.
unsolved: A binary variable indicating whether a homicide is unsolved 

```{r}

library(dplyr)

homicide_data <- read.csv("homicide-data.csv", stringsAsFactors = FALSE)


homicide_data <- homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", "))


homicide_data <- homicide_data %>%
  mutate(unsolved = disposition %in% c("Closed without arrest", "Open/No arrest"))


city_summary <- homicide_data %>%
  group_by(city_state) %>%
  summarise(
    total_homicides = n(),
    unsolved_homicides = sum(unsolved, na.rm = TRUE)
  )


print(city_summary)

```
```{r}

library(dplyr)
library(broom)


baltimore_data <- homicide_data %>%
  filter(city == "Baltimore", state == "MD")


unsolved_count <- sum(baltimore_data$unsolved, na.rm = TRUE)
total_count <- nrow(baltimore_data)


prop_test_result <- prop.test(x = unsolved_count, n = total_count)


tidy_result <- broom::tidy(prop_test_result)


estimated_proportion <- tidy_result$estimate
confidence_interval <- tidy_result[c("conf.low", "conf.high")]


list(estimated_proportion = estimated_proportion, confidence_interval = confidence_interval)

```

```{r}

library(dplyr)
library(purrr)
library(broom)
library(tidyr)


city_summary <- homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", "),
         unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")) %>%
  group_by(city_state) %>%
  summarise(
    unsolved_count = sum(unsolved, na.rm = TRUE),
    total_count = n()
  ) %>%
  ungroup()


prop_test_results <- city_summary %>%
  mutate(
    test_result = map2(unsolved_count, total_count, ~ prop.test(x = .x, n = .y) %>% tidy())
  ) %>%
  unnest(test_result)


city_proportions <- prop_test_results %>%
  select(city_state, estimate, conf.low, conf.high)


print(city_proportions)

```


```{r}
ggplot(city_proportions, aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.2) +
  coord_flip() +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  theme_minimal(base_size = 12) +  # Increase font size for readability
  theme(
    axis.text.y = element_text(size = 8),  # Adjust city label size
    plot.title = element_text(hjust = 0.5),  # Center the title
    panel.grid.major.y = element_blank()  # Reduce grid lines for cleaner look
  )

```

